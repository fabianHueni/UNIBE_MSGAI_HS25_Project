{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analysation of latency data\n",
    "In this notebook we want to analyse all the latency data\n",
    "\n",
    "\n",
    "\n",
    "### what do i want to analyse?\n",
    "- distribution of latency\n",
    "- correlation between input size and latency measures"
   ],
   "id": "cd56d4f1e3bcdd7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "2b30f996595e02b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ],
   "id": "adab5e47530f6f66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1) Load Dataset",
   "id": "9e22f2ad8ed5c788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define root results directory\n",
    "results_dir = Path('../results')\n",
    "\n",
    "# Find all files containing \"stats\" and ending with .csv in all subdirectories\n",
    "stats_files = sorted(results_dir.glob('**/*stats*.csv'))\n",
    "\n",
    "# Find all files containing \"raw\" and ending with .csv in all subdirectories\n",
    "raw_files = sorted(results_dir.glob('**/*raw*.csv'))\n",
    "\n",
    "print(f\"Found {len(stats_files)} stats CSV files and {len(raw_files)} raw CSV files:\")\n",
    "\n",
    "# Load all stats files into a dictionary of dataframes\n",
    "stats_dfs = {}\n",
    "for file_path in stats_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Strip whitespace from column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        # Create key: subfolder_name/filename_stem\n",
    "        relative_path = file_path.relative_to(results_dir)\n",
    "        key = str(relative_path.parent / relative_path.stem)\n",
    "        stats_dfs[key] = df\n",
    "        print(f\"✅ {relative_path} -> shape {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path.name}: {e}\")\n",
    "\n",
    "# Load all raw files into a dictionary of dataframes\n",
    "raw_dfs = {}\n",
    "for file_path in raw_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Strip whitespace from column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        # Create key: subfolder_name/filename_stem\n",
    "        relative_path = file_path.relative_to(results_dir)\n",
    "        key = str(relative_path.parent / relative_path.stem)\n",
    "        raw_dfs[key] = df\n",
    "        print(f\"✅ {relative_path} -> shape {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(stats_dfs)} stats dataframes and {len(raw_dfs)} raw dataframes ✅\")"
   ],
   "id": "56d65ec99374182e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# raw_dfs['fabian\\\\raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-03T20-41-45']",
   "id": "d33c8408c9c0ddce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check the keys in raw_dfs\n",
    "print(\"Available keys in raw_dfs:\")\n",
    "for key in raw_dfs.keys():\n",
    "    print(key)"
   ],
   "id": "dac57791f1cd510c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Inference Time vs. Accuracy",
   "id": "8d2afe7ca4f7ace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Call the function with the stats_dfs dictionary\n",
    "# TODO: shorten the labels (maybe just \"granite micro\" instead of ibm-granite-granite-4-0-h-micro). Would do this with a lookup table. Maybe add directly on data loading step.\n",
    "# TODO: color the dots based on cloud vs. on device\n",
    "# TODO: optional: make the size of the dots depending on the model size\n",
    "scatterplot_inference_vs_accuracy(stats_dfs)"
   ],
   "id": "f4c789c1312f3c04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot_time_measure_distributions(raw_dfs['fabian/raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-03T20-58-00'], 'Llama Fabian')\n",
    "# plot_time_measure_distributions(raw_dfs['philip\\\\raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T08-10-53'], 'Llama Philip')\n",
    "#plot_time_measure_distributions(raw_dfs['nicolas/lenovo_büro_raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T13-03-58'], 'Llama Nicolas Büro')\n"
   ],
   "id": "2a511751f06e6a6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the data for plotting\n",
    "experiment_data = [\n",
    "    raw_dfs['fabian/raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-03T20-41-45'],\n",
    "    raw_dfs['philip/raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-04T08-01-13'],\n",
    "    raw_dfs['nicolas/lenovo_büro_raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-04T12-44-34']\n",
    "]\n",
    "\n",
    "labels = ['gemma Fabian', 'gemma Philip', 'gemma Nicolas Büro']\n",
    "\n",
    "# plot time measure distributions\n",
    "plot_time_measures_overlaid(experiment_data, labels)\n",
    "\n",
    "# plot inference vs input character amount\n",
    "plot_characters_vs_inference_time(experiment_data, labels)"
   ],
   "id": "7b7ec00ed3b924c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the data for plotting\n",
    "experiment_data = [\n",
    "    raw_dfs['fabian/raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-03T20-58-00'],\n",
    "    raw_dfs['philip/raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T08-10-53'],\n",
    "    raw_dfs['nicolas-office/lenovo_büro_raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T13-03-58'],\n",
    "    raw_dfs['cloud/cloud_raw_experiment_meta-llama-llama-3-2-1b-instruct_always_cloud_once-per-sec_2025-12-04T16-10-54']\n",
    "]\n",
    "\n",
    "labels = ['Llama Fabian', 'Llama Philip', 'Llama Nicolas Büro', 'cloud']\n",
    "\n",
    "# plot time measure distributions\n",
    "plot_time_measures_overlaid(experiment_data, labels)\n",
    "\n",
    "# plot inference vs input character amount\n",
    "plot_characters_vs_inference_time(experiment_data, labels)"
   ],
   "id": "64ef1ef58ae20344"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the data for plotting\n",
    "experiment_data_granite = [\n",
    "    raw_dfs['fabian/raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-03T22-46-10'],\n",
    "    # Updated key for Fabian\n",
    "    raw_dfs['philip/raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-04T09-03-29'],\n",
    "    # Correct key for Philip\n",
    "    raw_dfs[\n",
    "        'nicolas/lenovo_büro_raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-04T12-29-10']\n",
    "    # Correct key for Nicolas\n",
    "]\n",
    "\n",
    "labels_granite = ['Granite Fabian', 'Granite Philip', 'Granite Nicolas Büro']\n",
    "\n",
    "# Plot time measure distributions for Granite experiments\n",
    "plot_time_measures_overlaid(experiment_data_granite, labels_granite)\n",
    "\n",
    "# Plot inference vs input character amount for Granite experiments\n",
    "plot_characters_vs_inference_time(experiment_data_granite, labels_granite)"
   ],
   "id": "87b754d5c94bbc88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b3d790c2e951561"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## n) Analyisis of Latency per Model and Device\n",
    "This analyiss is used to show the mean and median latency as well as the variance in latency for the on device models of different configurations. Those numbers can be copied to the report."
   ],
   "id": "f1510a581e9c03cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# filter raw_df such that only entries with fabian in the path are kept\n",
    "fabian_raw_dfs = {key: df for key, df in raw_dfs.items() if 'fabian' in key}"
   ],
   "id": "d13e3f14352d2856"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_latency_statistics_for_one_device(name: str, route: str = 'device') -> pd.DataFrame:\n",
    "    # filter all the raw dataframes which contain name in their key (e.g. 'fabian' for all datasets from fabian)\n",
    "    subset_raw_df = {key: df for key, df in raw_dfs.items() if name in key}\n",
    "\n",
    "    latency_stats = {}\n",
    "    for key, df in subset_raw_df.items():\n",
    "        if 'inference_time_ms' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        series = df['inference_time_ms'].dropna()\n",
    "        mean_latency = series.mean()\n",
    "        median_latency = series.median()\n",
    "        std_latency = series.std()\n",
    "        q25 = series.quantile(0.25)\n",
    "        q75 = series.quantile(0.75)\n",
    "        iqr_latency = q75 - q25\n",
    "\n",
    "        mean_accuracy = df['exact_match'].mean() if 'exact_match' in df.columns else None\n",
    "        device_model = df['device_model'][0]\n",
    "        cloud_model = df['cloud_model'][0]\n",
    "\n",
    "        model_name = cloud_model if route == 'cloud' else device_model\n",
    "\n",
    "        latency_stats[model_name] = {\n",
    "            'mean_latency_ms': round(mean_latency, 2),\n",
    "            'median_latency_ms': round(median_latency, 2),\n",
    "            'std_latency_ms': round(std_latency, 2),\n",
    "            'iqr_latency_ms': round(iqr_latency, 2),\n",
    "            'q25_latency_ms': round(q25, 2),\n",
    "            'q75_latency_ms': round(q75, 2),\n",
    "            'mean_accuracy': round(mean_accuracy, 2),\n",
    "            'dataset_name': key\n",
    "        }\n",
    "\n",
    "    # convert to a dataframe\n",
    "    return pd.DataFrame.from_dict(latency_stats, orient='index')\n"
   ],
   "id": "9fcc480a8b362cd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print the dataframe\n",
    "analyze_latency_statistics_for_one_device('fabian')"
   ],
   "id": "200c3728a1816f56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "analyze_latency_statistics_for_one_device('philip')",
   "id": "c0bbadc354f07514"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "analyze_latency_statistics_for_one_device('nicolas-privat')",
   "id": "e6841205d5418ef6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "analyze_latency_statistics_for_one_device('nicolas-office')",
   "id": "e63f5b44f664b4ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Prompt Characters vs. Latency Analysis\n",
    "This analysis is used to show the correlation between input size (in tokens) and latency for different models and devices."
   ],
   "id": "8c84e8ddbf2c3b30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the data for plotting\n",
    "experiment_data = [\n",
    "    [\n",
    "        raw_dfs['fabian\\\\raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-03T20-58-00'],\n",
    "        raw_dfs['philip\\\\raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T08-10-53'],\n",
    "        raw_dfs[\n",
    "            'nicolas-privat\\\\lenovo_privat_raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-05T08-43-10'],\n",
    "        raw_dfs[\n",
    "            'cloud\\\\cloud_raw_experiment_meta-llama-llama-3-2-1b-instruct_always_cloud_once-per-sec_2025-12-04T16-10-54']\n",
    "    ],\n",
    "    [\n",
    "        raw_dfs['fabian\\\\raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-03T22-46-10'],\n",
    "        raw_dfs['philip\\\\raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-04T09-03-29'],\n",
    "        raw_dfs[\n",
    "            'nicolas-privat\\\\lenovo_privat_raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-05T09-19-31'],\n",
    "        raw_dfs[\n",
    "            'cloud\\\\cloud_raw_experiment_ibm-granite-granite-4-0-h-micro_always_cloud_once-per-sec_2025-12-05T07-07-33']\n",
    "    ],\n",
    "    [\n",
    "        raw_dfs['fabian\\\\raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-03T20-41-45'],\n",
    "        raw_dfs['philip\\\\raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-04T08-01-13'],\n",
    "        raw_dfs[\n",
    "            'nicolas-privat\\\\lenovo_privat_raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-05T08-27-21']\n",
    "    ],\n",
    "    [\n",
    "        raw_dfs['fabian\\\\raw_experiment_Qwen3-4B-ONNX_always_device_once-per-sec_2025-12-06T10-14-33'],\n",
    "        raw_dfs['philip\\\\raw_experiment_Qwen3-4B-ONNX_always_device_once-per-sec_2025-12-04T10-12-42'],\n",
    "        raw_dfs[\n",
    "            'nicolas-privat\\\\lenovo_privat_raw_experiment_Qwen3-4B-ONNX_always_device_once-per-sec_2025-12-05T10-40-39']\n",
    "    ]\n",
    "\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    ['Asus Zenbook', 'Macbook Air', 'Lenovo', 'Cloud'],\n",
    "    ['Asus Zenbook', 'Macbook Air', 'Lenovo', 'Cloud'],\n",
    "    ['Asus Zenbook', 'Macbook Air', 'Lenovo'],\n",
    "    ['Asus Zenbook', 'Macbook Air', 'Lenovo']\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    'Llama-3-2-1B-Instruct-ONNX',\n",
    "    'Granite-4-0-micro-ONNX-web',\n",
    "    'Gemma-3-270m-it-ONNX',\n",
    "    'Qwen3-4B-ONNX'\n",
    "]\n",
    "\n",
    "\n",
    "# plot inference vs input character amount\n",
    "plot_characters_vs_inference_time(experiment_data, labels, model_names)"
   ],
   "id": "16abdfe19d51d3e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Correlation Analysis between Input Characters and Inference Time for Llama:\")\n",
    "calc_correlation_characters_inference(experiment_data[0], labels[0])"
   ],
   "id": "f288a41bdf588f9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Correlation Analysis between Input Characters and Inference Time for Granite:\")\n",
    "calc_correlation_characters_inference(experiment_data[1], labels[1])"
   ],
   "id": "e9df962bb50b7b13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Correlation Analysis between Input Characters and Inference Time for Gemma:\")\n",
    "calc_correlation_characters_inference(experiment_data[2], labels[2])"
   ],
   "id": "f72933a3c4fc2c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Correlation Analysis between Input Characters and Inference Time for Qwen:\")\n",
    "calc_correlation_characters_inference(experiment_data[3], labels[3])"
   ],
   "id": "980881c3ee98cad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) Distribution of Inference Time\n",
    "This section explores how the inference time is distributed for different models and devices."
   ],
   "id": "c546afadc02b1c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_inference_time_distribution(experiment_data, labels, model_names)",
   "id": "a983fa359f8bb95a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) Analysis of Warmup Effects\n",
    "This section analyzes whether there are warmup effects in the latency data, i.e., whether the latency decreases over time as the model \"warms up\"."
   ],
   "id": "169ff7acd5ab5165"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# select from each experiment the first 50 and last 50 samples\n",
    "def analyze_warmup_effects(experiment_data, labels):\n",
    "    for data_list, label_list in zip(experiment_data, labels):\n",
    "        for df, label in zip(data_list, label_list):\n",
    "            # Sort by job_start_ts to ensure chronological order\n",
    "            df_sorted = df.sort_values('job_start_ts').reset_index(drop=True)\n",
    "            first = df_sorted['inference_time_ms'].iloc[0]\n",
    "            second = df_sorted['inference_time_ms'].iloc[1]\n",
    "            diff_first_two = first - second\n",
    "\n",
    "            # get reduction of time in percentage\n",
    "            reduction_percentage = (diff_first_two / first) * 100\n",
    "\n",
    "            print(f\"{label}: Mean Inference Time - Difference in first two items: {diff_first_two:.2f} ms, {reduction_percentage:2f}% reduction\")"
   ],
   "id": "975b006bb9fd4e44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "analyze_warmup_effects(experiment_data, labels)",
   "id": "18a5eb02ea97cbf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_warm_up_phase(experiment_data, labels, model_names)",
   "id": "4e9b69b6fcd46fbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiment_data[1][3]",
   "id": "ee5a8e3cac808849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Queueing Models\n",
    "Here we try to simulate a queueing model that behaves like the empirically generated data. In a second step we then want to combine on-device and cloud into a multi-server model, to come up with a smart decision policy."
   ],
   "id": "cbfc6a5c6fa8c2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from utils import *",
   "id": "e16fea70ce869bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda_val, mu_val, s_times, ia_times = fit_mm1_model(\n",
    "    raw_dfs['philip/raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T08-10-53'])"
   ],
   "id": "b1caad437814d5f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Get rates from On-Device Experiment\n",
    "print(\"--- On-Device Data ---\")\n",
    "lambda_device, mu_device, _, _ = fit_mm1_model(\n",
    "    raw_dfs['philip/raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T08-10-53'])\n",
    "\n",
    "# 2. Get rates from Cloud Experiment\n",
    "print(\"\\n--- Cloud Data ---\")\n",
    "cloud_key = 'cloud/cloud_raw_experiment_meta-llama-llama-3-2-1b-instruct_always_cloud_once-per-sec_2025-12-04T16-10-54'\n",
    "lambda_cloud, mu_cloud, _, _ = fit_mm1_model(raw_dfs[cloud_key])\n",
    "\n",
    "# 3. Combine them into a 2-Server System\n",
    "# We assume the total arrival rate is the sum of both experiments (or you can set a target lambda)\n",
    "total_lambda = lambda_device + lambda_cloud\n",
    "\n",
    "print(\"\\n--- Combined System (M/M/2) ---\")\n",
    "# We pass the two different service rates\n",
    "fit_mmc_model(total_lambda, [mu_device, mu_cloud], c=2)"
   ],
   "id": "83cc9e2f07f2aae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Usage Example ---\n",
    "thresholds = range(0, 2000, 1)\n",
    "tolerance = 50      # how much slower (in ms) do we allow the on device model to be in comparison to the cloud model\n",
    "cost_device = 0.00\n",
    "cost_cloud = 0.01\n",
    "\n",
    "df_cloud_ex = raw_dfs['cloud/cloud_raw_experiment_meta-llama-llama-3-2-1b-instruct_always_cloud_once-per-sec_2025-12-04T16-10-54']\n",
    "df_device_ex = raw_dfs['philip/raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-04T08-01-13']\n",
    "\n",
    "_ = run_full_analysis(df_device_ex, df_cloud_ex, thresholds, tolerance, cost_device, cost_cloud)"
   ],
   "id": "f717236386cd30c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2228b7051af71c7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Coming up with a routing policy\n",
    "We want to find out the optimal Threshold T here for our \"smart\" scheduling policy. On the hardware we tested, mostly the cloud based inference was faster than on-device. We expect that in the future models will get faster and more people will have more access to higher performance hardware in their devices, thats why we lower the inference time for the on-device models by multiplying it with a `on_device_speedup_factor` and performing a linear shift with `on_device_speedup_shift`."
   ],
   "id": "d2a768f9ac0d3b48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "\n",
    "df_cloud_ex = raw_dfs['cloud/cloud_raw_experiment_meta-llama-llama-3-2-1b-instruct_always_cloud_once-per-sec_2025-12-04T16-10-54'].copy()\n",
    "df_device_ex = raw_dfs['philip/raw_experiment_Llama-3-2-1B-Instruct-ONNX_always_device_once-per-sec_2025-12-04T08-10-53'].copy()\n",
    "\n",
    "df_cloud_ex = raw_dfs['cloud/cloud_raw_experiment_ibm-granite-granite-4-0-h-micro_always_cloud_once-per-sec_2025-12-05T07-07-33'].copy()\n",
    "df_device_ex = raw_dfs['philip/raw_experiment_granite-4-0-micro-ONNX-web_always_device_once-per-sec_2025-12-04T09-03-29'].copy()\n",
    "\n",
    "df_cloud_ex = raw_dfs['cloud/cloud_raw_experiment_openai-gpt-4o-mini_always_cloud_once-per-sec_2025-12-05T07-18-02'].copy()\n",
    "df_device_ex = raw_dfs['philip/raw_experiment_gemma-3-270m-it-ONNX_always_device_once-per-sec_2025-12-04T08-01-13'].copy()\n",
    "\n",
    "\n",
    "\n",
    "on_device_speedup_factor = 1\n",
    "on_device_speedup_shift = 00 #linear shift factor in ms\n",
    "\n",
    "# Apply speedup factor to device inference times\n",
    "print(f\"Applying speedup factor {on_device_speedup_factor} and shift -{on_device_speedup_shift}ms...\")\n",
    "\n",
    "# clip(lower=1.0) to prevent negative inference times!\n",
    "df_device_ex['inference_time_ms'] = (df_device_ex['inference_time_ms'] * on_device_speedup_factor - on_device_speedup_shift).clip(lower=1.0)\n",
    "\n",
    "# WARNING: We cannot simply recalculate total_latency_ms here.\n",
    "# The old 'queueing_time_ms' is invalid because a faster device would have had a much smaller queue.\n",
    "# We must rely on the M/G/1 simulation to estimate the new total latency.\n",
    "\n",
    "# Recalculate total latency to maintain consistency (Total = Queue + Inference). ... (Don't do this if you want accurate total stats)\n",
    "# df_device_ex['total_latency_ms'] = df_device_ex['queueing_time_ms'] + df_device_ex['inference_time_ms']\n"
   ],
   "id": "8de77fda85f6a00d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now extract basic per-server metrics from the (manipulated) experiment data.",
   "id": "a9b61f9d5745d6d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "\n",
    "extract_basic_metrics(df_device_ex, \"On-Device (Llama)\")\n",
    "extract_basic_metrics(df_cloud_ex, \"Cloud (Llama)\")"
   ],
   "id": "234082c869a32efe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the basic per-server metrics we fit a queueing model per server using the measured service time distribution. To find out what queue model we should use. we need to know the distribution of the interarrival times and the distribution of the service times, therefore we plot them.",
   "id": "4e1a94177ac1755d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_inter_arrival_distribution(df_device_ex, \"On-Device (Llama)\")\n",
    "plot_inter_arrival_distribution(df_cloud_ex, \"Cloud (Llama)\")"
   ],
   "id": "e30ddb9685d20506"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plot_service_time_distribution(df_device_ex, \"On-Device (Llama)\")\n",
    "plot_service_time_distribution(df_cloud_ex, \"Cloud (Llama)\")"
   ],
   "id": "397fc1db8514946f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that the interarrival times are roughly deterministic. This matches our expectations, as we did the experiments with 1 request per second. The service time distribution plots show that the empirical service times are not clearly exponential, with different shapes and variances for device and cloud. Therefore, for analysis, we model our cloud and on-device systems as **G/G/1** queues (General Arrival, General Service).\n",
    "\n",
    "Since there is no exact closed-form solution for the mean waiting time in a G/G/1 queue, we use **Kingman's Approximation** (see lecture/literature). The expected waiting time $E[T_Q]$ is approximated as:\n",
    "\n",
    "$$\n",
    "E[T_Q] \\approx \\frac{\\rho}{1-\\rho} \\cdot \\frac{c_a^2 + c_s^2}{2} \\cdot E[S]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\rho = \\lambda E[S]$ is the utilization.\n",
    "- $E[S]$ is the mean service time.\n",
    "- $c_a$ is the coefficient of variation of inter-arrival times ($c_a = \\sigma_a / \\mu_a$).\n",
    "- $c_s$ is the coefficient of variation of service times ($c_s = \\sigma_s / \\mu_s$).\n",
    "\n",
    "This formula generalizes the queueing behavior:\n",
    "1.  **For our Experiment (D/G/1):** Since arrivals are deterministic, $c_a \\approx 0$. The waiting time is driven purely by the service variability ($c_s^2$).\n",
    "2.  **For M/G/1 (Theoretical):** If we assume random Poisson arrivals, $c_a = 1$. In this case, Kingman's formula simplifies back to the Pollaczek–Khinchine formula.\n",
    "\n",
    "Using this approximation, we can analytically compute the expected mean response time for both our specific experiment ($c_a=0$) and for a hypothetical real-world scenario with random user arrivals ($c_a=1$)."
   ],
   "id": "97e6c29cfc6abce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "\n",
    "# --- Run Analysis ---\n",
    "thresholds = range(0, 2500, 50)\n",
    "# ca=0.0 models your deterministic experiment (D/G/1)\n",
    "gg1_results = analyze_routing_gg1(df_device_ex, df_cloud_ex, thresholds, ca=0.0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gg1_results['threshold'], gg1_results['avg_latency'], label='G/G/1 Estimated Latency (Kingman)', linewidth=2)\n",
    "plt.axhline(y=gg1_results.iloc[0]['avg_latency'], color='gray', linestyle='--', label='All Cloud (Baseline)')\n",
    "\n",
    "finite_vals = gg1_results[gg1_results['avg_latency'] != float('inf')]['avg_latency']\n",
    "upper_lim = finite_vals.max() * 1.1 if not pd.isna(finite_vals.max()) else 2.0\n",
    "lower_lim = max(0, finite_vals.min() * 0.9) if not pd.isna(finite_vals.min()) else 0\n",
    "plt.ylim(lower_lim, upper_lim)\n",
    "\n",
    "plt.xlabel('Threshold (Characters)')\n",
    "plt.ylabel('Mean Response Time (s)')\n",
    "plt.title('G/G/1 Routing Analysis: Kingman\\'s Approximation (ca=0.0)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "afae919f4e9beced"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**how to interpret the analyis plot:**\n",
    "- X-Axis is the Threshold we set for the routing policy. Any prompt with less characters than T is sent to local device, anything longer goes to the cloud.\n",
    "- Y-Axis is the Mean Response Time $E[R]$ for the combined system (device and cloud). It's the weighted weighted average time of requests served by device and cloud.\n",
    "- The optimal Threshold is the minimum of the curve"
   ],
   "id": "9f5b75a13e838bef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**To validate the analytical G/G/1 setup (Kingman's approximation), we build a discrete‑event simulation.** We run two variations: one with **Deterministic arrivals** (matching our experiment, $c_a=0$) and one with **Poisson arrivals** (matching theoretical random traffic, $c_a=1$). For each arrival, we apply the routing rule (if size $\\le T$ route to device, otherwise to cloud) and simulate the two FCFS queues using empirically sampled service times. For each threshold $T$, we estimate the average response time over many simulated jobs (`num_jobs`) and compare it to the analytical curves. This verifies that the approximation holds for both our specific experimental conditions and general random workloads.",
   "id": "9a02a500d2769e96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "\n",
    "# --- Run Simulation ---\n",
    "# Use fewer thresholds or jobs if it's too slow\n",
    "sim_thresholds = range(0, 2500, 50)\n",
    "\n",
    "# Calculate Lambda once\n",
    "lambda_sim = calculate_system_arrival_rate(df_device_ex, df_cloud_ex)\n",
    "\n",
    "# Run Simulation with Deterministic Arrivals (ca=0.0)\n",
    "sim_results_det = simulate_routing_validation(df_device_ex, df_cloud_ex, sim_thresholds, lambda_sim, num_jobs=20000, ca=0.0)\n",
    "\n",
    "# Run Simulation with Poisson Arrivals (ca=1.0)\n",
    "sim_results_poisson = simulate_routing_validation(df_device_ex, df_cloud_ex, sim_thresholds, lambda_sim, num_jobs=20000, ca=1.0)\n",
    "\n",
    "# --- Plot Comparison ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Note: Using gg1_results from the previous cell (Analytical Model)\n",
    "plt.plot(gg1_results['threshold'], gg1_results['avg_latency'], label='Analytical (G/G/1 Kingman)', linewidth=2, color='blue')\n",
    "plt.plot(sim_results_poisson['threshold'], sim_results_poisson['sim_latency'], 'o--', label='Sim (Poisson, ca=1)', color='orange', alpha=0.5)\n",
    "plt.plot(sim_results_det['threshold'], sim_results_det['sim_latency'], 'x--', label='Sim (Deterministic, ca=0)', color='green')\n",
    "\n",
    "# Baseline\n",
    "plt.axhline(y=gg1_results.iloc[0]['avg_latency'], color='gray', linestyle='--', label='All Cloud (Baseline)')\n",
    "\n",
    "# Dynamic Limits\n",
    "finite_vals = gg1_results[gg1_results['avg_latency'] != float('inf')]['avg_latency']\n",
    "upper_lim = finite_vals.max() * 1.1 if not pd.isna(finite_vals.max()) else 2.0\n",
    "lower_lim = max(0, finite_vals.min() * 0.9) if not pd.isna(finite_vals.min()) else 0\n",
    "plt.ylim(lower_lim, upper_lim)\n",
    "\n",
    "plt.xlabel('Threshold (Characters)')\n",
    "plt.ylabel('Mean Response Time (s)')\n",
    "plt.title('Validation: G/G/1 Analytical Model vs. Discrete Event Simulation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "dac6bcafcad32019"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The discrete-event simulation shows slightly lower latency than the Kingman approximation. This is expected, as Kingman's formula is an upper-bound approximation that tends to overestimate waiting times for deterministic arrivals at non-saturated utilization levels.\n",
    "\n",
    "\n",
    "Now we want to look at a fully simulated version. Here we don't sample arrival- and inference time from our experiments, but we generate them artificially. Earlier we identified a linear relationship between input size and inference time, we will use this to generate accurate distributions to sample pairs of input sizes and inference times."
   ],
   "id": "e7c9bebab90b2701"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get parameters for Device\n",
    "slope_dev, int_dev = estimate_linear_relationship(df_device_ex, \"On-Device (Llama)\")\n",
    "\n",
    "# Get parameters for Cloud\n",
    "slope_cloud, int_cloud = estimate_linear_relationship(df_cloud_ex, \"Cloud (Llama)\")\n",
    "\n",
    "# Get parameters for Input Character Distribution\n",
    "char_mean = df_device_ex['number_of_characters'].mean()\n",
    "char_std = df_device_ex['number_of_characters'].std()\n",
    "\n",
    "print(f\"\\nSimulation Params -> Char: ({char_mean:.1f}, {char_std:.1f})\")"
   ],
   "id": "29eede0ac2e2a3cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now take the slope and intercept of the cloud and on-device models and use them to generate sample jobs for our simulation. We will generate Gaussian distributed input sizes and calculate the corresponding inference times using the linear models.",
   "id": "9644a9935c06efd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "\n",
    "# --- Run Simulation ---\n",
    "# Use fewer thresholds or jobs if it's too slow\n",
    "sim_thresholds = range(0, 2500, 50)\n",
    "\n",
    "# Calculate Lambda once\n",
    "lambda_sim = calculate_system_arrival_rate(df_device_ex, df_cloud_ex)\n",
    "print(lambda_sim)\n",
    "\n",
    "# Run Simulation with Deterministic Arrivals (ca=0.0)\n",
    "sim_results_det = simulate_routing_synthetic(thresholds, lambda_sim, num_jobs=10000, ca=0.0, char_params=(char_mean, char_std), dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud))\n",
    "\n",
    "# Run Simulation with Poisson Arrivals (ca=1.0)\n",
    "sim_results_poisson = simulate_routing_synthetic(thresholds, lambda_sim, num_jobs=10000, ca=1.0, char_params=(char_mean, char_std), dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud))\n",
    "\n",
    "# --- Plot Comparison ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Note: Using gg1_results from the previous cell (Analytical Model)\n",
    "plt.plot(gg1_results['threshold'], gg1_results['avg_latency'], label='Analytical (G/G/1 Kingman)', linewidth=2, color='blue')\n",
    "#plt.plot(sim_results_poisson['threshold'], sim_results_poisson['sim_latency'], 'o--', label='Sim (Poisson, ca=1)', color='orange', alpha=0.5)\n",
    "plt.plot(sim_results_det['threshold'], sim_results_det['sim_latency'], 'x--', label='Sim (Deterministic, ca=0)', color='green')\n",
    "\n",
    "# Baseline\n",
    "plt.axhline(y=gg1_results.iloc[0]['avg_latency'], color='gray', linestyle='--', label='All Cloud (Baseline)')\n",
    "plt.axhline(y=gg1_results.iloc[-1]['avg_latency'], color='red', linestyle=':', label='All Device (Baseline)')\n",
    "\n",
    "# Dynamic Limits\n",
    "finite_vals = gg1_results[gg1_results['avg_latency'] != float('inf')]['avg_latency']\n",
    "upper_lim = finite_vals.max() * 1.1 if not pd.isna(finite_vals.max()) else 2.0\n",
    "lower_lim = max(0, finite_vals.min() * 0.9) if not pd.isna(finite_vals.min()) else 0\n",
    "plt.ylim(lower_lim, upper_lim)\n",
    "\n",
    "plt.xlabel('Threshold (Characters)')\n",
    "plt.ylabel('Mean Response Time (s)')\n",
    "plt.title('Validation: G/G/1 Analytical Model vs. Discrete Event Simulation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "3591e2f410988a50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have successfully created a model where we can specify the relationship between input size and inference time for two G/G/1 systems, connected by a threshold-based scheduling policy. This allows us to simulate and optimize the system under various theoretical workloads (deterministic or poisson arrival) and hardware configurations.",
   "id": "2726c730d0cfca2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a range of arrival rates to test (e.g., 0.5 to 3.0 requests per second)\n",
    "#test_lambdas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0, 1.1, 1.2, 1.3, 1.4, 2]\n",
    "test_lambdas = [0.1,0.2,0.3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(test_lambdas)))\n",
    "\n",
    "print(\"Running simulations for different arrival rates...\")\n",
    "\n",
    "all_finite_latencies = []\n",
    "\n",
    "for i, lam in enumerate(test_lambdas):\n",
    "    print(f\"  -> Simulating λ = {lam:.1f} req/s\")\n",
    "\n",
    "    # Run synthetic simulation (using Poisson arrivals ca=1.0 for realistic random traffic)\n",
    "    sim_res = simulate_routing_synthetic(\n",
    "        thresholds,\n",
    "        lam,\n",
    "        num_jobs=10000,\n",
    "        ca=1.0, # 1.0 is poisson, 0.0 is deterministc\n",
    "        char_params=(char_mean, char_std),\n",
    "        dev_model=(slope_dev, int_dev),\n",
    "        cloud_model=(slope_cloud, int_cloud)\n",
    "    )\n",
    "\n",
    "    # Plot the curve\n",
    "    plt.plot(sim_res['threshold'], sim_res['sim_latency'],\n",
    "             label=f'λ = {lam:.1f}', color=colors[i], linewidth=2)\n",
    "\n",
    "    # Collect finite values for auto-scaling\n",
    "    finite_vals = sim_res[sim_res['sim_latency'] != float('inf')]['sim_latency']\n",
    "    all_finite_latencies.extend(finite_vals.dropna().tolist())\n",
    "\n",
    "plt.xlabel('Threshold (Characters)')\n",
    "plt.ylabel('Mean Response Time (s)')\n",
    "plt.title('Impact of Arrival Rate (λ) on Optimal Routing Threshold')\n",
    "plt.legend(title=\"Arrival Rate (req/s)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Set reasonable Y-limits to ignore unstable/infinite queues\n",
    "if all_finite_latencies:\n",
    "    upper_lim = max(all_finite_latencies) * 1.1\n",
    "    lower_lim = max(0, min(all_finite_latencies) * 0.9)\n",
    "    plt.ylim(lower_lim, upper_lim)\n",
    "\n",
    "plt.show()"
   ],
   "id": "504e96f2c42574da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a range of arrival rates to test (e.g., 0.5 to 3.0 requests per second)\n",
    "#test_lambdas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0, 1.1, 1.2, 1.3, 1.4, 2]\n",
    "test_lambdas = [0.1,0.2,0.3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(test_lambdas)))\n",
    "\n",
    "print(\"Running simulations for different arrival rates...\")\n",
    "\n",
    "all_finite_latencies = []\n",
    "\n",
    "for i, lam in enumerate(test_lambdas):\n",
    "    print(f\"  -> Simulating λ = {lam:.1f} req/s\")\n",
    "\n",
    "    # Run synthetic simulation (using Poisson arrivals ca=1.0 for realistic random traffic)\n",
    "    sim_res = simulate_routing_synthetic(\n",
    "        thresholds,\n",
    "        lam,\n",
    "        num_jobs=10000,\n",
    "        ca=0.0, # 1.0 is poisson, 0.0 is deterministc\n",
    "        char_params=(char_mean, char_std),\n",
    "        dev_model=(slope_dev, int_dev),\n",
    "        cloud_model=(slope_cloud, int_cloud)\n",
    "    )\n",
    "\n",
    "    # Plot the curve\n",
    "    plt.plot(sim_res['threshold'], sim_res['sim_latency'],\n",
    "             label=f'λ = {lam:.1f}', color=colors[i], linewidth=2)\n",
    "\n",
    "    # Collect finite values for auto-scaling\n",
    "    finite_vals = sim_res[sim_res['sim_latency'] != float('inf')]['sim_latency']\n",
    "    all_finite_latencies.extend(finite_vals.dropna().tolist())\n",
    "\n",
    "plt.xlabel('Threshold (Characters)')\n",
    "plt.ylabel('Mean Response Time (s)')\n",
    "plt.title('Impact of Arrival Rate (λ) on Optimal Routing Threshold')\n",
    "plt.legend(title=\"Arrival Rate (req/s)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Set reasonable Y-limits to ignore unstable/infinite queues\n",
    "if all_finite_latencies:\n",
    "    upper_lim = max(all_finite_latencies) * 1.1\n",
    "    lower_lim = max(0, min(all_finite_latencies) * 0.9)\n",
    "    plt.ylim(lower_lim, upper_lim)\n",
    "\n",
    "plt.show()"
   ],
   "id": "81be4447030d623d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e8d4eb9fff942cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Deriving an input size based policy to send to device or cloud",
   "id": "8dfda6074eebbdac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 1. Load the dataset\n",
    "# Assuming the notebook is in 'analyse/' and the dataset is in 'dataset/' relative to the project root\n",
    "dataset_path = Path('../dataset/boolq_validation.csv')\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(f\"⚠️ File not found at {dataset_path}. Please check the path.\")\n",
    "else:\n",
    "    df_requests = pd.read_csv(dataset_path)\n",
    "    print(f\"✅ Loaded {len(df_requests)} requests from {dataset_path.name}\")\n",
    "\n",
    "    # 2. Initialize Estimator\n",
    "    # Using a 5-second window to react relatively quickly to changes in our simulation loop\n",
    "    estimator = TrafficRateEstimator(window_size_seconds=5)\n",
    "\n",
    "    print(\"\\n--- Streaming Requests from Dataset ---\")\n",
    "\n",
    "    # 3. Iterate through requests\n",
    "    for i, row in df_requests.head(20).iterrows():\n",
    "\n",
    "        # Calculate input size (Question + Passage)\n",
    "        input_text = str(row['question']) + \" \" + str(row['passage'])\n",
    "        input_len = len(input_text)\n",
    "\n",
    "        # Register the arrival of this request\n",
    "        estimator.register_request()\n",
    "\n",
    "        # Get current load estimate\n",
    "        current_lambda = estimator.get_current_lambda()\n",
    "\n",
    "        # Make routing decision\n",
    "        decision, opt_threshold, _ = recommend_routing_decision(\n",
    "            input_len,\n",
    "            current_lambda,\n",
    "            (char_mean, char_std),\n",
    "            (slope_dev, int_dev),\n",
    "            (slope_cloud, int_cloud)\n",
    "        )\n",
    "\n",
    "        print(f\"Req {i+1:02d}: Size={input_len:4d} chars | Est. λ={current_lambda:.2f} | Threshold={opt_threshold} -> Decision: {decision}\")\n",
    "\n",
    "        # Simulate variable traffic load with BURSTS\n",
    "        # Every 20 requests, switch between \"High Traffic\" (burst) and \"Low Traffic\" (lull)\n",
    "        if (i // 20) % 2 == 0:\n",
    "            # High Traffic Mode: Fast arrivals (0.01s - 0.2s) -> High Lambda\n",
    "            time.sleep(random.uniform(0.01, 0.1))\n",
    "        else:\n",
    "            # Low Traffic Mode: Slow arrivals (0.5s - 1.5s) -> Low Lambda\n",
    "            time.sleep(random.uniform(0.5, 1.5))"
   ],
   "id": "d4a2c1dcc9d54120"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The experiment above shows something that is not fulfilled in our policy: the scheduler does not keep track of how many elements are present in which queue. to avoid overfilling one queue while the other system is idling, we should keep track of how many requests are in the queues of cloud and on device",
   "id": "c41ddff17edd8025"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### including states to keep track of queue lengths\n",
    "the scheduler is effectively asking: \"Including the current backlog, which server will finish this specific job faster?\" This is the core principle of the \"Join the Shortest Expected Queue\" (JSEQ) policy"
   ],
   "id": "2edef151d2cfc35d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# join shortest queue size\n",
    "from utils import *\n",
    "\n",
    "# 1. Load the dataset\n",
    "df_requests = pd.read_csv(dataset_path)\n",
    "print(f\"✅ Loaded {len(df_requests)} requests from {dataset_path.name}\")\n",
    "\n",
    "# 2. Initialize Stateful Scheduler with our device/cloud performance models\n",
    "# Ensure slope_dev, int_dev, etc. have been defined in a previous cell\n",
    "scheduler = StatefulScheduler(dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud))\n",
    "\n",
    "print(\"\\n--- Streaming Requests with Stateful JSEQ Scheduler (Advanced Simulation) ---\")\n",
    "\n",
    "# --- Advanced Simulation Setup ---\n",
    "# We get rid of time.sleep() and manage time manually for a more accurate simulation.\n",
    "sim_time = 0.0  # This is our simulation's internal clock\n",
    "total_latency = 0.0\n",
    "num_requests_processed = 0\n",
    "\n",
    "# 3. Iterate through requests\n",
    "for i, row in df_requests.head(500).iterrows():\n",
    "\n",
    "    # Determine next arrival time based on traffic mode\n",
    "    if (i // 20) % 2 == 0: # Burst mode\n",
    "        arrival_delay = random.uniform(0.005, 0.15)\n",
    "    else: # Lull mode\n",
    "        arrival_delay = random.uniform(0.8, 1.5)\n",
    "\n",
    "    # Advance simulation time to the next arrival\n",
    "    sim_time += arrival_delay\n",
    "\n",
    "    # Calculate input size\n",
    "    input_text = str(row['question']) + \" \" + str(row['passage'])\n",
    "    input_len = len(input_text)\n",
    "\n",
    "    # Make a state-aware routing decision using the simulation's clock\n",
    "    decision, start_time, finish_time = scheduler.decide_at_time(input_len, arrival_time=sim_time)\n",
    "\n",
    "    # Calculate and record stats for this request\n",
    "    queue_time = start_time - sim_time\n",
    "    service_time = finish_time - start_time\n",
    "    response_time = finish_time - sim_time # This is queue_time + service_time\n",
    "\n",
    "    total_latency += response_time\n",
    "    num_requests_processed += 1\n",
    "\n",
    "    # Convert to milliseconds for more readable output\n",
    "    queue_time_ms = queue_time * 1000\n",
    "    service_time_ms = service_time * 1000\n",
    "\n",
    "    # Use '<7' to left-align the decision string in a 7-character space\n",
    "    print(f\"Req {i+1:03d} (t={sim_time:6.2f}s): Size={input_len:4d} -> {decision:<7} | Queue: {queue_time_ms:6.1f}ms, Inference: {service_time_ms:6.1f}ms\")\n",
    "\n",
    "print(f\"\\n--- Simulation Complete ---\")\n",
    "print(f\"Average Response Time over {num_requests_processed} requests: {total_latency / num_requests_processed:.4f}s\")"
   ],
   "id": "57dcd12b4973576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "# --- 1. SETUP: Ensure all parameters from previous cells are available ---\n",
    "# dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud)\n",
    "# char_params=(char_mean, char_std)\n",
    "# dataset_path\n",
    "\n",
    "# --- 2. GENERATE A CONSISTENT REQUEST STREAM ---\n",
    "# Create a fixed list of requests and arrival times to ensure every policy is tested on the exact same workload.\n",
    "print(\"--- Generating a consistent once-per-second request stream for simulation ---\")\n",
    "df_requests = pd.read_csv(dataset_path)\n",
    "num_sim_requests = 500\n",
    "request_stream = []\n",
    "\n",
    "# The loop index 'i' will serve as the arrival time in seconds (0, 1, 2, ...)\n",
    "for i, row in df_requests.head(num_sim_requests).iterrows():\n",
    "    arrival_time = float(i)\n",
    "\n",
    "    input_text = str(row['question']) + \" \" + str(row['passage'])\n",
    "    input_len = len(input_text)\n",
    "\n",
    "    request_stream.append({'id': i, 'arrival_time': arrival_time, 'size': input_len})\n",
    "\n",
    "# --- 3. DEFINE A GENERIC SIMULATION RUNNER ---\n",
    "def run_policy_simulation(scheduler, requests):\n",
    "    \"\"\"Runs a simulation for a given scheduler and request stream.\"\"\"\n",
    "    total_latency = 0.0\n",
    "    for req in requests:\n",
    "        _, _, finish_time = scheduler.decide_at_time(req['size'], req['arrival_time'])\n",
    "        response_time = finish_time - req['arrival_time']\n",
    "        total_latency += response_time\n",
    "\n",
    "    return total_latency / len(requests) if requests else 0.0\n",
    "\n",
    "# --- 4. PRE-CALCULATE OPTIMAL THRESHOLD FOR THE STATELESS POLICY ---\n",
    "# Calculate the average lambda of our generated stream to find a reasonable fixed threshold.\n",
    "avg_inter_arrival_time = request_stream[-1]['arrival_time'] / len(request_stream)\n",
    "avg_lambda = 1.0 / avg_inter_arrival_time\n",
    "print(f\"\\nAverage arrival rate (λ) for the stream: {avg_lambda:.2f} req/s\")\n",
    "\n",
    "# Find the optimal threshold for this average lambda\n",
    "thresholds = range(0, 2500, 1)\n",
    "sim_results = simulate_routing_synthetic(\n",
    "    thresholds, avg_lambda, num_jobs=5000, ca=1.0,\n",
    "    char_params=(char_mean, char_std),\n",
    "    dev_model=(slope_dev, int_dev),\n",
    "    cloud_model=(slope_cloud, int_cloud)\n",
    ")\n",
    "# Find the threshold that gives the minimum latency\n",
    "optimal_T_stateless = sim_results.loc[sim_results['sim_latency'].idxmin()]['threshold']\n",
    "print(f\"Optimal fixed threshold (T) for stateless policy: {optimal_T_stateless} characters\")\n",
    "\n",
    "# 7. Add Cost Info Text\n",
    "info_text = (f\"Cost Settings:\\n\"\n",
    "             f\"Device: ${cost_device_per_req:.2f}/req\\n\"\n",
    "             f\"Cloud:  ${cost_cloud_per_req:.2f}/req\")\n",
    "ax1.text(0.8, 0.95, info_text, transform=ax1.transAxes,\n",
    "         fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# --- 5. INITIALIZE ALL SCHEDULERS ---\n",
    "# We use the StatefulScheduler class for all, but with routing logic overridden for simpler policies.\n",
    "# This ensures they all share the same underlying queue management mechanics.\n",
    "\n",
    "# a) Stateful JSEQ Scheduler (no forced policy)\n",
    "scheduler_jseq = StatefulScheduler(dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud))\n",
    "\n",
    "# b) Always-Cloud Scheduler (forced policy)\n",
    "scheduler_cloud = StatefulScheduler(dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud), force_policy='cloud')\n",
    "\n",
    "# c) Always-Device Scheduler (forced policy)\n",
    "scheduler_device = StatefulScheduler(dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud), force_policy='device')\n",
    "\n",
    "# d) Stateless Threshold Scheduler\n",
    "# For this one, we still need to override the main decision logic, as it's a unique policy.\n",
    "scheduler_stateless = StatefulScheduler(dev_model=(slope_dev, int_dev), cloud_model=(slope_cloud, int_cloud))\n",
    "# Override the main decision method to implement the fixed threshold logic\n",
    "scheduler_stateless.decide_at_time = lambda size, time: scheduler_stateless.route_to_device(size, time) if size <= optimal_T_stateless else scheduler_stateless.route_to_cloud(size, time)\n",
    "\n",
    "\n",
    "# --- 6. RUN SIMULATIONS AND GATHER RESULTS ---\n",
    "print(\"\\n--- Running simulations for all policies ---\")\n",
    "results = {}\n",
    "\n",
    "results['Always Device'] = run_policy_simulation(scheduler_device, request_stream)\n",
    "print(f\"Finished: Always Device\")\n",
    "\n",
    "results['Always Cloud'] = run_policy_simulation(scheduler_cloud, request_stream)\n",
    "print(f\"Finished: Always Cloud\")\n",
    "\n",
    "results['Stateless Threshold'] = run_policy_simulation(scheduler_stateless, request_stream)\n",
    "print(f\"Finished: Stateless Threshold (T={optimal_T_stateless})\")\n",
    "\n",
    "results['Stateful JSEQ'] = run_policy_simulation(scheduler_jseq, request_stream)\n",
    "print(f\"Finished: Stateful JSEQ\")"
   ],
   "id": "19f60b5300d5cc07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 7. ADD EMPIRICAL INFERENCE TIMES ---\n",
    "# Calculate the average inference times from the original dataframes\n",
    "avg_device_inference_ms = df_device_ex['total_latency_ms'].mean()\n",
    "avg_cloud_inference_ms = df_cloud_ex['total_latency_ms'].mean()\n",
    "\n",
    "# Add them to the results dictionary, converting from ms to seconds\n",
    "results['Measured Device Total Latency'] = avg_device_inference_ms / 1000.0\n",
    "results['Measured Cloud Total Latency'] = avg_cloud_inference_ms / 1000.0\n",
    "\n",
    "\n",
    "# --- 8. PLOT AND DISPLAY RESULTS ---\n",
    "print(\"\\n--- Simulation Results ---\")\n",
    "for policy, latency in results.items():\n",
    "    print(f\"{policy:<30}: {latency:.4f}s average response time\")\n",
    "\n",
    "# Define the categories for the x-axis\n",
    "categories = ['Always Device', 'Always Cloud', 'Stateless Threshold', 'Stateful JSEQ']\n",
    "x = np.arange(len(categories))  # the label locations\n",
    "\n",
    "# Data for the simulated response time bars\n",
    "sim_latencies = {\n",
    "    'Always Device': results['Always Device'],\n",
    "    'Always Cloud': results['Always Cloud'],\n",
    "    'Stateless Threshold': results['Stateless Threshold'],\n",
    "    'Stateful JSEQ': results['Stateful JSEQ']\n",
    "}\n",
    "# Data for the measured total latency bars\n",
    "measured_latencies = {\n",
    "    'Always Device': results['Measured Device Total Latency'],\n",
    "    'Always Cloud': results['Measured Cloud Total Latency']\n",
    "}\n",
    "\n",
    "bar_width = 0.35  # Width of each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# --- Plotting the bars ---\n",
    "ax.bar(x[0] - bar_width/2, sim_latencies['Always Device'], bar_width,\n",
    "       label='Simulated Response Time always Device', color='#ff9999')\n",
    "ax.bar(x[0] + bar_width/2, measured_latencies['Always Device'], bar_width,\n",
    "       label='Measured total Latency always Device', color='#e60000')\n",
    "ax.bar(x[1] - bar_width/2, sim_latencies['Always Cloud'], bar_width,\n",
    "       label='Simulated Response Time always Device', color='#66b3ff')\n",
    "ax.bar(x[1] + bar_width/2, measured_latencies['Always Cloud'], bar_width,\n",
    "       label='Measured total Latency always Cloud', color='#0059b3')\n",
    "ax.bar(x[2], sim_latencies['Stateless Threshold'], bar_width,\n",
    "       label='Simulated Response Time Threshold Scheduler', color='#99ff99')\n",
    "ax.bar(x[3], sim_latencies['Stateful JSEQ'], bar_width,\n",
    "       label='Simulated Response Time Stateful JSEQ', color='#ffcc99')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Average Time (s)')\n",
    "ax.set_title('Comparison of Scheduling Policies: Simulated vs. Measured Latency')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Function to attach a text label above each bar\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}s',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "# Label all bars plotted\n",
    "autolabel(ax.patches)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f9eddc54f8a60e41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_jseq_ex = raw_dfs['philip/raw_experiment_gemma-3-270m-it-ONNX_openai-gpt-4o-mini_jseq_once-per-sec_2025-12-08T17-12-38'].copy()\n",
    "\n",
    "# --- 7. ADD EMPIRICAL INFERENCE TIMES ---\n",
    "# Calculate the average inference times from the original dataframes\n",
    "avg_device_inference_ms = df_device_ex['total_latency_ms'].mean()\n",
    "avg_cloud_inference_ms = df_cloud_ex['total_latency_ms'].mean()\n",
    "avg_jseq_inference_ms = df_jseq_ex['total_latency_ms'].mean()\n",
    "\n",
    "# Add them to the results dictionary, converting from ms to seconds\n",
    "results['Measured Device Total Latency'] = avg_device_inference_ms / 1000.0\n",
    "results['Measured Cloud Total Latency'] = avg_cloud_inference_ms / 1000.0\n",
    "results['Measured JSEQ Total Latency'] = avg_jseq_inference_ms / 1000.0\n",
    "\n",
    "\n",
    "# --- 8. PLOT AND DISPLAY RESULTS ---\n",
    "print(\"\\n--- Simulation Results ---\")\n",
    "for policy, latency in results.items():\n",
    "    print(f\"{policy:<30}: {latency:.4f}s average response time\")\n",
    "\n",
    "# Define the categories for the x-axis\n",
    "categories = ['Always Device', 'Always Cloud', 'Stateless Threshold', 'Stateful JSEQ']\n",
    "x = np.arange(len(categories))  # the label locations\n",
    "\n",
    "# Data for the simulated response time bars\n",
    "sim_latencies = {\n",
    "    'Always Device': results['Always Device'],\n",
    "    'Always Cloud': results['Always Cloud'],\n",
    "    'Stateless Threshold': results['Stateless Threshold'],\n",
    "    'Stateful JSEQ': results['Stateful JSEQ']\n",
    "}\n",
    "# Data for the measured total latency bars\n",
    "measured_latencies = {\n",
    "    'Always Device': results['Measured Device Total Latency'],\n",
    "    'Always Cloud': results['Measured Cloud Total Latency'],\n",
    "    'Stateful JSEQ': results['Measured JSEQ Total Latency']\n",
    "}\n",
    "\n",
    "bar_width = 0.35  # Width of each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# --- Plotting the bars ---\n",
    "ax.bar(x[0] - bar_width/2, sim_latencies['Always Device'], bar_width,\n",
    "       label='Simulated Response Time always Device', color='#ff9999')\n",
    "ax.bar(x[0] + bar_width/2, measured_latencies['Always Device'], bar_width,\n",
    "       label='Measured total Latency always Device', color='#e60000')\n",
    "ax.bar(x[1] - bar_width/2, sim_latencies['Always Cloud'], bar_width,\n",
    "       label='Simulated Response Time always Device', color='#66b3ff')\n",
    "ax.bar(x[1] + bar_width/2, measured_latencies['Always Cloud'], bar_width,\n",
    "       label='Measured total Latency always Cloud', color='#0059b3')\n",
    "ax.bar(x[2], sim_latencies['Stateless Threshold'], bar_width,\n",
    "       label='Simulated Response Time Threshold Scheduler', color='#99ff99')\n",
    "ax.bar(x[3] - bar_width/2, sim_latencies['Stateful JSEQ'], bar_width,\n",
    "       label='Simulated Response Time Stateful JSEQ', color='#ffcc99')\n",
    "ax.bar(x[3] + bar_width/2, measured_latencies['Stateful JSEQ'], bar_width,\n",
    "       label='Measured total Latency stateful JSEQ', color=\"#e67b00\")\n",
    "\n",
    "\n",
    "# Add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Average Time (s)')\n",
    "ax.set_title('Comparison of Scheduling Policies: Simulated vs. Measured Latency')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Function to attach a text label above each bar\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}s',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "# Label all bars plotted\n",
    "autolabel(ax.patches)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a10792a43fc0be3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c2b65ff9bc3808c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
